

# **Video Analytics Using Spark Streaming**

# **Lab Overview**

Spark Streaming is a key component of Spark, a hugely successful open project in the recent years. Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map-reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards.

In this lab we will integrate Spark Streaming and Kafka, detect a human face using OpenCV from a video streaming, and then save the results to a file in Hadoop environment.

Our application is will run a video data ingestion using a micro service. This micro service is running continuously. It will read the video data stream from Camera and forward the data to Kafka broker. Our sample code will read the video data from Kafka broker and detect the face image and save that image into HDFS platform. As micro-service is running continuously, user need not to start it separately. User can view the image from DLP platform.

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/video-image-capturing.png?raw=true)

## **Lab Objectives**

- Learn how to integrate the SparkStreaming with Kafka
- Learn how to perform video analytic functions in a real-time stream application
- Learn how to save a file to HDFS

## **Prerequisites**

- Knowledge of Java language, Maven
- Knowledge for OpenCV face detection function
- Studied platform user guide
- Basic knowledge of how Spark Streaming works
- Use Chrome Browser


## Step 1: Explore Data Learning Platform(DLP)

<font color='red'>Request access to the Data Learning Platform by sending a message to:</font> [datalearningplatform@cisco.com](mailto:datalearningplatform@cisco.com)

1. In DLP website, go to the development area by clicking <b>"Development Hub"</b>. 
2. Select the workspace called "wksp-video-lab" and Click on <b>"Launch"</b> button.
3. It will open a new browser tab with pre-loaded video streaming code.

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/WelComeScreenVideoStreaming.PNG?raw=true)

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/VideoStreamingCodeInEditor.PNG?raw=true)
N.B: This sample code can be used for exploring the process.


## Step 2: Sample Code for Detecting face from live Stream Camera.
<b>N.B: Our video streaming progam is used to detect the video streaming from a source. Below Java Program is used to detect the video stream from Camera and detect the face and save the image of the face in HDFS.</b>

```json
package DDP.VideoLabs;

import java.io.BufferedInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.URI;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import org.joda.time.DateTime;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;

import org.opencv.core.Core;
import org.opencv.core.Mat;
import org.opencv.core.MatOfRect;
import org.opencv.core.Size;
import org.opencv.imgcodecs.Imgcodecs;
import org.opencv.imgproc.Imgproc;
import org.opencv.objdetect.CascadeClassifier;

import kafka.serializer.DefaultDecoder;
import kafka.serializer.StringDecoder;
import scala.Tuple2;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

public class App 
{
    public static void main( String[] args ) throws InterruptedException
    {
    	if (args.length != 3)
    	{
    		System.out.println("please set parameter value for kafka and topic and username");
    		return;
    	}
    	String kafka = args[0];
        String topics = args[1];
        final String username = args[2];
        
      	// Create context with a 2 seconds batch interval
        SparkConf sparkConf = new SparkConf().setAppName("JavaVideoLabs");
        final JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));

        HashSet<String> topicsSet = new HashSet<String>(Arrays.asList(topics.split(",")));
        HashMap<String, String> kafkaParams = new HashMap<String, String>();
        kafkaParams.put("metadata.broker.list", kafka);
        kafkaParams.put("group.id", "groupid");
        kafkaParams.put("consumer.id", "consumerid");

        // Create direct kafka stream with brokers and topics
        JavaPairInputDStream<String, byte[]> messages = KafkaUtils.createDirectStream(
                jssc,
                String.class,
                byte[].class,
                StringDecoder.class,
                DefaultDecoder.class,
                kafkaParams,
                topicsSet
        );
        

                  
        JavaDStream<String> content = messages.map(new Function<Tuple2<String, byte[]>, String>() {
           	private static final long serialVersionUID = 1L;
			//@Override
            public String call(Tuple2<String, byte[]> tuple2) throws IOException {
                System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
                if ((tuple2 == null) || (tuple2._2().length < 1000))
                	return "";
                
                Mat image = new Mat(new Size(640, 480), 16);
                image.put(0, 0, tuple2._2()); 
                                    
                // Detect faces in the image.
                Mat mGrey = new Mat();
                Imgproc.cvtColor( image, mGrey, Imgproc.COLOR_BGR2GRAY); 
                CascadeClassifier faceDetector =
                        new CascadeClassifier(GetResourceFilePath("/haarcascade_frontalface_alt.xml").toString());
                MatOfRect faceDetections = new MatOfRect();
                faceDetector.detectMultiScale(mGrey, faceDetections);
                int len = faceDetections.toArray().length;
                System.out.println(String.format("Detected %s faces", len));
                if (len > 0)
                {
	                SaveImageToHDFS(image, username);
	                return "face";
	              
                } else
                    return "";
           
            }
        });
        
        DateTime start = new DateTime();
        
        content.count().print();
       
        jssc.start();
        DateTime end;
        while (true) {
            end = new DateTime();
            if (end.getMillis()-start.getMillis() > 60000) {
                jssc.stop(true, true);
                break;
            }

        }

        jssc.awaitTermination();
    }
    
    public static void SaveImageToHDFS(Mat img, String username) throws IOException
    {
    	 DateTime now = new DateTime();
    	 int year = now.getYear();
    	 int month = now.getMonthOfYear();
    	 int day = now.getDayOfMonth();
    	 int hour = now.getHourOfDay();
    	 int minute = now.getMinuteOfHour();
    	 int second = now.getSecondOfMinute();
    	 String time = ""+year+month+day+hour+minute+second;
    	 String tmpFile = "/tmp/" + username + time + ".jpg";
         Imgcodecs.imwrite(tmpFile, img);
         String hdfsAddr = "hdfs://172.16.1.11:8020/ddp/"+username+"/"+time+".jpg";
         Configuration config = new Configuration();
         FileSystem fs = FileSystem.get(URI.create(hdfsAddr), config);
         Path path = new Path(hdfsAddr);
         FSDataOutputStream out = fs.create(path);
         InputStream is = new BufferedInputStream(new FileInputStream(tmpFile));
         IOUtils.copyBytes(is, out, config);
         is.close();
         out.close();
         fs.close();
         
         File f = new File(tmpFile);
     	 if (f.exists() && !f.isDirectory())
     		f.delete();
         System.out.println("write file to hdfs");
    }
    
    public static String GetResourceFilePath(String filename) {
        InputStream inputStream = null;
        OutputStream outputStream = null;
        String tempFilename = "/tmp" + filename;
        try {
        	File f = new File(tempFilename);
        	if (f.exists() && !f.isDirectory())
        		return tempFilename;
        	
            // read this file into InputStream
            inputStream = App.class.getResourceAsStream(filename);
            if (inputStream == null)
                System.out.println("empty streaming");
            // write the inputStream to a FileOutputStream
            outputStream =
                    new FileOutputStream(tempFilename);

            int read;
            byte[] bytes = new byte[102400];

            while ((read = inputStream.read(bytes)) != -1) {
                outputStream.write(bytes, 0, read);
            }
            outputStream.flush();

            System.out.println("Load XML file, Done!");

        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            if (inputStream != null) {
                try {
                    inputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
            if (outputStream != null) {
                try {
                    // outputStream.flush();
                    outputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }

            }

        }
        return tempFilename;
    }
    
}


```   

## Step 3: Maven package and submit the spark task to Hadoop cluster.

Once we finish the above code, we can select <b>“package”</b> in the CMD menu, and click on Blue Button ![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/BlueButton.PNG?raw=true). This will package the application.

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/BuildProject.PNG?raw=true)

After successfull packaging, select  <b>“run”</b>  click the blue button again.

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/runProject.PNG?raw=true)


## Step 4: View the file which has face images in DLP Platform

After executing the run.sh file successfully, image file will be generated and saved into hadoop file system. You can view it in DLP follow the below step.

1. From the DLP home page, go to the <b>Data Repository</b>.
2. Find the image file in the data list.
3. Select the image file, then click it, you can preview the image.  
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/previrewImage.PNG?raw=true)
4. After clickng on the preview button, image would be display. Loading of the imagem would take some time based on the internet speed.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/PreviewImage2.PNG?raw=true)


**Input** : The video streaming data from DLP.  
**Output** : An image file which has face in HDFS, and you can preview it in DLP.  

From this we have learned how to how to integrate the SparkStreaming with Kafka and how to save the file to HDFS.

There are some more examples and exercise are available in the below mentioned link. This is for your reference.

[http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)

