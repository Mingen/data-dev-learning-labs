# <center>Word Count Using SPARK</center>

## Lab Overview

WordCount is a classic example of learning Spark programming language. From our Data Learning Platform(DLP), user can  write code using Spark programming language, execute the program and view the output on Hadoop platform. Main aim of this platform is to use Hadoop environment for writing and executeing Spark programming language and view the output using data files directly from the platform.

After completing this lab exercise, user can try their own spark program using different data file from Hadoop environment.

<font color='red'>Request access to the Data Learning Platform by sending a message to:</font> [datalearningplatform@cisco.com](mailto:datalearningplatform@cisco.com)

## Lab Objectives

* Learn how to get data from a file.
* Learn how to use some RDD operations on data.
* Learn how to write data to a new file with file creation in HDFS.

## Prerequisites

* Knowledge on Hadoop to store the data.
* Basic knowledge of how spark works.
* Chrome Browser

## Step 1: Explore Data Learning Platform(DLP)
DLP platform is pre-populated with workspace for executing different programing language. For using the Spark programming language, user need to select the below workspace.

1)	After logging on DLP, click on <b>Development Hub</b> on the left column.<br>
2)	Select the pre-defined work space named as <b>wksp-scala</b>.<br>
3)	Click on the <b>Launch</b> button to open IDE workspace.<br>

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/SelectWorkSpace.PNG?raw=true)

## Step 2: Explore IDE Workspace
For writing any programming language, DLP platform is pre-loaded with an IDE. User can write Spark programing language in this IDE. </br>
Below sections will help to explore the IDE for Word Count project(wordcount). In the IDE, files are listed in Left Panel.
Double click on WordCount.scala file. Left Panel will be populated with the code of this file. 

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/WordCountScalafile.PNG?raw=true)


<b> Pre-loaded raw data file named(wordcountinputfile1.txt) in HDFS environment.</b><br>
This program will count the number of words in the file by identifying spaces between words. This file is already pre-loaded in hadoop environment. 
N.B: We use a sample file with small data set(wordcountinputfile1.txt) containing pre-populated words pre-loaded in Hadoop. Spark program will exeecute on this sample data file and execute the occurance of each word and show the output.

``` json
//Import libraries which are needed to run the program. 
import org.apache.spark.{SparkContext, SparkConf}
object WordCount
{
  def main(args: Array[String]) {
    val inputFile = args(0)
    val outputFile = args(1)
    val conf = new SparkConf().setAppName("wordCount")
    // Create a Scala Spark Context.
    val sc = new SparkContext(conf)
    // Load our input data.
    val input = sc.textFile(inputFile)
    // Split up into words.
    val words = input.flatMap(line => line.split(" "))
    // Transform into word and count.
    val counts = words.map(word => (word, 1)).reduceByKey{_ + _}
    // Save the word count back out to a text file, causing evaluation.
    counts.saveAsTextFile(outputFile)
  }
}
```
N.B: Use WordCount.scala to view the above code. 

# Step 3 :  Run the Program

1) Select the WordCountRun.sh file from left panel of the IDE and double click on it. </br>
This step will show the process of Running the Spark program from the IDE. Program will be compiled from Hadoop platfrom.</br>

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Step7.PNG?raw=true)

<br>
There are two parameters. <br>
a) Input Parameter: Here, input file from hdfs path is "wordcountinputfile1.txt"<br>
b) Output Parameter: Here, output file which would be saved in hdfs.
<br>
  Input file: hdfs://172.16.1.11:8020/tmp/spark/input/wordcountinputfile1.txt.<br>
  N.B: This is pre-loaded input file. <br>
  Output file: hdfs://172.16.1.11:8020/tmp/spark/output/output47<br>
  <b>Change the output file name(e.g: <b>output47</b>) based on your choice.</b>
<br>
This is the configuration file of setting all the parameters. </br>

2) Double click on WordCound.scala file again. Change <b>"package"</b> as described in the below image and click on the <b>"run"</b> blue button. This process will build and package the program.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/buildWordCount.PNG?raw=true)

3) You can find the below screen shot if build process finished successfully.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/buildSuccessWordCount.PNG?raw=true)

4) If the build process finished successfully, then select <b>"run"</b> as described in the below image and click on <b>blue run</b> button. 
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/runWordCount.PNG?raw=true)

5) Successful run process will show the output as below. (There should not be any Exception message in the exeception window if run process finished successfully.)
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/successRunProcess.PNG?raw=true)

5) After successful ran process, select "view" as shown the below screen and click on blue button. It will display the word count as below.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/FinalWordCount.PNG?raw=true)
Count of occurance of every word is showing in the output window.


An Example:
Input file has the text: <b>welcome to data data learning platform cisco</b>


Things to Try:

* Try with numeric data type
* Try with case sensitive data as well.

Completing this coding exercise, we have learned how to count the number of words in an input file using Spark Batch Processing. <br>

There are some more examples and exercise are available in the below mentioned link. This is for your reference.
[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html).
