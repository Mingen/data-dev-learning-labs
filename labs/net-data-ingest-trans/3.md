# Step 3: View Data in Visualization Tool
Below step will describe the process of visualizing the network data usig Tableau. 
Switch to <b>Data Repository</b> tab and select your output file. This output file is the file name which we mentioned in run.sh file.
![alt tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/visuaNetworkData.PNG?raw=true)
After selecting the respective file, click on <b></>Drill</b> button to visualize the data in Tableau. Tableau will be open in a new tab.

# Step 4: Login in Tablue
On clicking on the <b></>Drill</b> button, Tableau login screen will open. Enter user name as your login user id. For example, if you login as user_1 then login if would be here as <b>user_1</b> and password is <b>devnet</b>
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/tableauLogin.PNG?raw=true)
</br>
<b>Please allow browser's popup blocker to access Tableau UI.</b>

# Step 4: Tableau Interface.
User can select the respective fields and view the data in graphical format here. For example, user can select ip_src and ip_dest fields from Dimensions column and place it as showen in the below screen shot. Accordingly, change the Marks as Circle. It will generate the output which will describe that each dot marks is showing that there is a traffic between two devices. Device's IP address is marked as X-axis and Y-Axis.
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/tableauUIOnNetworkData.PNG?raw=true)

</br>
User can select any other fields based on their own choice to display the output. 


Sample Code: Transform a network data file and store transformed file to HDFS.

Below is the sample code which is used for getting the csv file of network data from HDFS environment, perform the transformation and store the transformed data into HDFS. 

```java
//Import the below libraries which are used to run the code.
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;

import java.util.Arrays;
//Class: TransformData. A class that transform the network data
public class TransformData {

    public static void main(String [] args) throws Exception {


  //three parameters here
        String filePath= args[0];
        String outputPath= args[1];
        String distFilePath= args[2];

        final String seperatorField = ",";

        //System.out.println("master address:"+master);
        System.out.println("input file path:"+filePath);
// here we are giving application name as “File transform”
        SparkConf conf = new SparkConf().setAppName("File transform");
        JavaSparkContext sc = new JavaSparkContext(conf);

        JavaRDD<String> textFile = sc.textFile(filePath);

        JavaRDD<String> cols = textFile.flatMap(new FlatMapFunction<String, String>() {
            public Iterable<String> call(String s) {
                String [] splits = s.split(seperatorField) ;

                String extractedCols = "";

                for(int i= 1 ;i<= 6 ;i ++ ){
                    extractedCols += splits[i ] + seperatorField ;
                }

                if(extractedCols.endsWith(seperatorField)){
                    extractedCols = extractedCols.substring(0,extractedCols.length() - 1)  ;
                }

                return Arrays.asList(extractedCols);
            }
        });


       cols.coalesce(1).saveAsTextFile(outputPath);

       boolean bool = getMergeInHdfs(outputPath,distFilePath);
        System.out.println("merge success:"+bool);



    }

    public static boolean getMergeInHdfs(String src, String dest) throws IllegalArgumentException, Exception {

        Configuration config = new Configuration();
        FileSystem fs = FileSystem.get(config);
        Path srcPath = new Path(src);
        Path dstPath = new Path(dest);

        // Check if the path already exists
        if (!(fs.exists(srcPath))) {
            System.out.println("Path " + src + " does not exists!");
            return false;
        }

        if ((fs.exists(dstPath))) {
            System.out.println("File Path " + dest + " exists!");
            return false;
        }

        return FileUtil.copyMerge(fs, srcPath, fs, dstPath, true, config, null);
    }
}
```
Network data ingestion process is allowing you to store any IoT data directly into DLP platform. Developer can use these data for further process and developing application.
